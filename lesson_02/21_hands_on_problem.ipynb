{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on 1: Ingestion and Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "Given the Paul Grahamâ€™s essay, build a sophisticated chat-engine address this target conversation:â€‹\n",
    "\n",
    "- Primary query: â€œWhat did Paul Graham do in the summer 1995?â€â€‹\n",
    "- Follow-up query: â€œWhy did he organize it?â€â€‹\n",
    "\n",
    "### Suggested tasks\n",
    "- ðŸ” Use the previous QueryFusionRetrieveâ€‹\n",
    "- ðŸ”¤ Create a ChatMemoryBufferâ€‹\n",
    "- ðŸš€ Create a CondensePlusContextChatEngineâ€‹\n",
    "- ðŸ“Š Validate the result!â€‹\n",
    "- ðŸŒ [+] Create a Chat & QueryGeneration prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running on colab uncomment those lines\n",
    "%pip install httpx==0.27.2\n",
    "%pip install llama-index==0.12.3\n",
    "%pip install llama-index-retrievers-bm25>=0.50\n",
    "%pip install openai==1.57.0\n",
    "!mkdir -p 'data/paul_graham/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load .env\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from rich import print as rprint\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.retrievers.fusion_retriever import FUSION_MODES\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.chat_engine.condense_plus_context import CondensePlusContextChatEngine\n",
    "from llama_index.core.base.base_retriever import BaseRetriever\n",
    "from llama_index.core.base.base_query_engine import BaseQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(\"./data/paul_graham\").load_data()\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=256)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, transformations=[splitter], show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_GEN_PROMPT=\"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. Generate {num_queries} search queries, one on each line, related to the following input query:\n",
    "Query: {query}\n",
    "Queries:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_query_fusion_retriever(index: VectorStoreIndex, similarity_top_k: int, mode: FUSION_MODES, num_queries:int = 3) -> QueryFusionRetriever:\n",
    "    # todo: write here your implementation\n",
    "    # create a vector retriever\n",
    "    # create a bm25 retriever\n",
    "    # create a QueryFusionRetriever with the vector and bm25 retrievers\n",
    "    raise NotImplementedError(\"QueryFusionRetriever is not implemented yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_PROMPT = \"\"\"You're a chatbot, able to have normal conversations, about an essay discussing Paul Grahams life. \n",
    "Here are the relevant documents for the context:\n",
    "{context_str}\n",
    "Instruction: use the previous chat history, or the context aboute, to interact with the user. Use only the information provided in the context to generate responses.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_condense_plus_context_chat_engine(retriever: BaseRetriever, query_engine: BaseQueryEngine) -> CondensePlusContextChatEngine:\n",
    "    # todo: write here your implementation\n",
    "    # create a ChatMemoryBuffer\n",
    "    # create a CondensePlusContextChatEngine with the retriever, query_engine, memory\n",
    "    raise NotImplementedError(\"CondensePlusContextChatEngine is not implemented yet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = init_query_fusion_retriever(index, similarity_top_k=5, mode=FUSION_MODES.RECIPROCAL_RANK, num_queries=3)\n",
    "query_engine = RetrieverQueryEngine(retriever)\n",
    "chat_engine = init_condense_plus_context_chat_engine(retriever=retriever, query_engine=query_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat_engine.chat(\"What did Paul Graham do in the summer of 1995?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine.chat(\"Why did he organize it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine.chat(\"What did I ask you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to make more questions or just try to use QUERY_GEN_PROMPT and CHAT_PROMPT and see how it goes..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
