{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on 2: Metadata & Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Problem\n",
    "\n",
    "Create a function with those requirements:â€‹\n",
    "- âš¡ Input: path of paul_graham_essay.txtâ€‹\n",
    "- ðŸ”„ Output: index able to answer correctly to these questions: â€‹\n",
    "    - \"Who is the author of the book?â€‹\"\n",
    "    - \"What inspired the author to switch from studying philosophy to studying AI in college?â€‹\"\n",
    "    - \"What would the author say about art vs. Engineering?â€‹\"\n",
    "    - \"Why did the author have to learn Italian?â€‹\"\n",
    "    - \"Why the author was in Florence?â€‹\"\n",
    "\n",
    "### ðŸ” Suggested tasks\n",
    "- ðŸ“¥ Ingest text from file with the right readerâ€‹\n",
    "- ðŸ”Ž Filter useless information â€‹\n",
    "- âœ‚ï¸ Split the page content with the best parserâ€‹\n",
    "- ðŸ“‹ Extract metadata for each chunkâ€‹\n",
    "- ðŸ—ï¸ Build the indexâ€‹\n",
    "- âœ… Test the index on provided questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p 'data/'\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'data/paul_graham_essay.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index>=0.11.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must be run before any asyncio code is run\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "from rich import print as rprint\n",
    "import os\n",
    "# add your imports here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"here your openai api key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_index(document_path: str) -> VectorStoreIndex:\n",
    "    # add your code here...\n",
    "    # 1. create a pipeline\n",
    "    # 2. load the documents\n",
    "    # 3. run the pipeline\n",
    "    # 4. create the index\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_path = \"TODO\" \n",
    "\n",
    "index = create_index(document_path)\n",
    "\n",
    "engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(engine.query(\"Who is the author of the book?\").response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(engine.query(\"What inspired the author to switch from studying philosophy to studying AI in college?\").response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(engine.query(\"What would the author say about art vs. engineering?\").response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(engine.query(\"Why did the author have to learn italian?\").response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rprint(engine.query(\"Why the author was in Florence?\").response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
